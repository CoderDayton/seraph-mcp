# ============================================================================
# Seraph MCP — Environment Configuration (Example)
# ============================================================================
# Copy this file to .env and customize with your values.
#
# This file follows the SDD.md canonical configuration specification.
# See docs/SDD.md for detailed documentation on all settings.
#
# Quick Start:
#   1. Copy: cp .env.example .env
#   2. Set ENVIRONMENT (development|staging|production)
#   3. Choose CACHE_BACKEND (memory|redis)
#   4. If redis: Set REDIS_URL
#   5. Start: fastmcp dev src/server.py
# ============================================================================

# ============================================================================
# CORE CONFIGURATION
# ============================================================================

# Runtime environment: development, staging, production
# - development: verbose logging, relaxed validation
# - staging: production-like with extra debugging
# - production: strict validation, minimal logging, security enforced
ENVIRONMENT=development

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# CACHE CONFIGURATION (Core Feature)
# ============================================================================
# Cache backend selection: memory | redis
# - memory: In-process LRU cache (default, zero dependencies)
# - redis: Redis-backed cache (requires REDIS_URL)
CACHE_BACKEND=memory

# Cache namespace: prefix for all cache keys
CACHE_NAMESPACE=seraph

# Default TTL in seconds (0 = no expiry)
CACHE_TTL_SECONDS=3600

# Memory backend settings (only applies when CACHE_BACKEND=memory)
# Maximum number of entries before LRU eviction
CACHE_MAX_SIZE=1000

# ----------------------------------------------------------------------------
# Redis Backend Settings (required when CACHE_BACKEND=redis)
# ----------------------------------------------------------------------------
# Uncomment and set these when using CACHE_BACKEND=redis:

# Redis connection URL
# Examples:
#   - Local: redis://localhost:6379/0
#   - With auth: redis://:password@localhost:6379/0
#   - TLS: rediss://localhost:6380/0
#   - Sentinel: redis+sentinel://sentinel1:26379,sentinel2:26379/myservice/0
# REDIS_URL=redis://localhost:6379/0

# Redis connection pool size
# REDIS_MAX_CONNECTIONS=10

# Redis socket timeout in seconds
# REDIS_SOCKET_TIMEOUT=5

# ----------------------------------------------------------------------------
# Toggle Cache Backend Examples
# ----------------------------------------------------------------------------
# To switch from memory to Redis:
#   1. Change CACHE_BACKEND=memory to CACHE_BACKEND=redis
#   2. Uncomment and set REDIS_URL=redis://your-redis-host:6379/0
#   3. Optionally tune REDIS_MAX_CONNECTIONS and REDIS_SOCKET_TIMEOUT
#   4. Restart the server
#
# Memory cache (default):
#   CACHE_BACKEND=memory
#   CACHE_MAX_SIZE=1000
#
# Redis cache (toggle on):
#   CACHE_BACKEND=redis
#   REDIS_URL=redis://localhost:6379/0
#   REDIS_MAX_CONNECTIONS=20
#   REDIS_SOCKET_TIMEOUT=5

# ============================================================================
# OBSERVABILITY & MONITORING
# ============================================================================

# Observability backend: simple | prometheus | datadog
# - simple: Structured logging + in-memory metrics (default)
# - prometheus: Prometheus metrics exporter (requires plugin)
# - datadog: Datadog integration (requires plugin)
OBSERVABILITY_BACKEND=simple

# Enable metrics collection (true|false)
ENABLE_METRICS=true

# Enable distributed tracing (true|false)
ENABLE_TRACING=false

# Metrics port (for prometheus exporter, if enabled)
METRICS_PORT=9090

# Prometheus metrics path
PROMETHEUS_PATH=/metrics

# ----------------------------------------------------------------------------
# Datadog Settings (required when OBSERVABILITY_BACKEND=datadog)
# ----------------------------------------------------------------------------
# Uncomment when using Datadog:
# DATADOG_API_KEY=your-datadog-api-key-here
# DATADOG_SITE=datadoghq.com

# ============================================================================
# OPTIMIZATION CONFIGURATION (Plugin-Provided)
# ============================================================================
# These settings require optimization plugins to be installed and enabled.

# Enable automatic optimization (true|false)
ENABLE_OPTIMIZATION=false

# Optimization strategy: balanced | cost | quality
OPTIMIZATION_MODE=balanced

# Minimum quality preservation ratio (0.0-1.0)
QUALITY_THRESHOLD=0.90

# Maximum optimization overhead in milliseconds
MAX_OVERHEAD_MS=100.0

# ============================================================================
# BUDGET ENFORCEMENT (Plugin-Provided)
# ============================================================================
# These settings require budget enforcement plugins to be installed.

# Enable budget limits (true|false)
ENABLE_BUDGET_ENFORCEMENT=false

# Daily budget limit in USD (optional)
# DAILY_BUDGET_LIMIT=5.0

# Monthly budget limit in USD (optional)
# MONTHLY_BUDGET_LIMIT=100.0

# Budget alert thresholds (comma-separated, 0.0-1.0)
BUDGET_ALERT_THRESHOLDS=0.5,0.75,0.9

# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================

# Enable authentication (true|false)
# Note: MCP client-side authentication is recommended
ENABLE_AUTH=false

# API keys for plugin HTTP adapters (comma-separated)
# Only needed if plugins provide HTTP endpoints
# API_KEYS=key1,key2,key3

# Allowed host headers (comma-separated, * = all)
ALLOWED_HOSTS=*

# ============================================================================
# DEVELOPMENT & TESTING
# ============================================================================

# Debug mode (verbose output, may expose sensitive info)
DEBUG=false

# Enable hot reload for development
# WATCHFILES_ENABLED=true

# ============================================================================
# PRODUCTION RECOMMENDATIONS
# ============================================================================
# When deploying to production:
#
# 1. ENVIRONMENT=production
# 2. LOG_LEVEL=WARNING or ERROR
# 3. ENABLE_METRICS=true (for monitoring)
# 4. CACHE_BACKEND=redis (for multi-instance deployments)
# 5. REDIS_URL with TLS (rediss://) and authentication
# 6. Use secret management (Vault, cloud secrets) instead of .env
# 7. Set ALLOWED_HOSTS to specific domains (not *)
# 8. Enable ENABLE_AUTH if exposing HTTP endpoints via plugins
# 9. Configure OBSERVABILITY_BACKEND=datadog or prometheus
# 10. Set reasonable budget limits if using cost-sensitive plugins
#
# Production example:
#   ENVIRONMENT=production
#   LOG_LEVEL=WARNING
#   CACHE_BACKEND=redis
#   REDIS_URL=rediss://:secure-password@prod-redis.example.com:6380/0
#   REDIS_MAX_CONNECTIONS=50
#   OBSERVABILITY_BACKEND=datadog
#   DATADOG_API_KEY=<from-secret-manager>
#   ENABLE_METRICS=true
#   ENABLE_TRACING=true

# ============================================================================
# ADVANCED / OPTIONAL SETTINGS
# ============================================================================

# Custom configuration providers (experimental)
# CONFIG_PROVIDERS=env,file,vault
# CONFIG_FILE_PATH=./config/app.yaml
# CONFIG_VAULT_URL=https://vault.example.com

# Performance tuning (experimental)
# CONFIG_CACHE_ENABLED=true
# CONFIG_CACHE_TTL=300
# CONFIG_VALIDATION_STRICT=true

# ============================================================================
# NOTES
# ============================================================================
# - All settings are optional; defaults are defined in src/config/schemas.py
# - Settings are loaded in order: code defaults → .env → environment variables
# - Environment variables take precedence over .env file values
# - Never commit .env to source control (it's in .gitignore)
# - Use CACHE_BACKEND=memory for local development (no dependencies)
# - Use CACHE_BACKEND=redis for production (persistent, shared cache)
# - Redis client (redis-py v4+) is included in core dependencies
# - Plugins may introduce additional environment variables
# - See docs/SDD.md for the canonical configuration reference

# ============================================================================
# AI MODEL PROVIDERS
# ============================================================================
# Configure API providers for model routing and AI capabilities.
# Only configure the providers you plan to use.
# All providers are disabled by default until API keys are provided.

# ----------------------------------------------------------------------------
# OpenAI Provider
# ----------------------------------------------------------------------------
# Enable OpenAI GPT models (gpt-4, gpt-3.5-turbo, etc.)
OPENAI_ENABLED=false
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...
OPENAI_TIMEOUT=30.0
OPENAI_MAX_RETRIES=3

# ----------------------------------------------------------------------------
# Anthropic Provider
# ----------------------------------------------------------------------------
# Enable Anthropic Claude models (claude-3-opus, claude-3-sonnet, etc.)
ANTHROPIC_ENABLED=false
# Get your API key from: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_TIMEOUT=30.0
ANTHROPIC_MAX_RETRIES=3

# ----------------------------------------------------------------------------
# Google Gemini Provider
# ----------------------------------------------------------------------------
# Enable Google Gemini models (gemini-pro, gemini-1.5-pro, etc.)
GEMINI_ENABLED=false
# Get your API key from: https://makersuite.google.com/app/apikey
# GEMINI_API_KEY=AIza...
GEMINI_TIMEOUT=30.0
GEMINI_MAX_RETRIES=3

# ----------------------------------------------------------------------------
# OpenAI-Compatible Provider
# ----------------------------------------------------------------------------
# Enable custom OpenAI-compatible endpoints (LM Studio, Ollama, vLLM, etc.)
OPENAI_COMPATIBLE_ENABLED=false
# Base URL for your custom endpoint (required)
# Examples:
#   - LM Studio: http://localhost:1234/v1
#   - Ollama: http://localhost:11434/v1
#   - vLLM: http://your-server:8000/v1
# OPENAI_COMPATIBLE_BASE_URL=http://localhost:1234/v1
# API key (optional, some services don't require it)
# OPENAI_COMPATIBLE_API_KEY=not-needed
OPENAI_COMPATIBLE_TIMEOUT=30.0
OPENAI_COMPATIBLE_MAX_RETRIES=3

# ----------------------------------------------------------------------------
# Provider Configuration Tips
# ----------------------------------------------------------------------------
# - Enable only the providers you have API keys for
# - Each provider can be toggled independently
# - Timeout and retry settings can be tuned per provider
# - OpenAI-compatible provider allows using local/self-hosted models
# - Keep API keys secure and never commit them to source control

# ============================================================================
# MODELS.DEV INTEGRATION
# ============================================================================
# All model information (pricing, capabilities, context windows) is dynamically
# loaded from the Models.dev API (https://models.dev/api.json).
#
# Benefits:
# - Always up-to-date pricing and model information
# - No hardcoded model data
# - Access to 1000+ models across 50+ providers
# - Automatic discovery of new models
#
# The Models.dev API is queried automatically when needed, with responses
# cached for 1 hour to minimize API calls.

# ============================================================================
# SEMANTIC CACHE CONFIGURATION
# ============================================================================
# Semantic caching uses vector similarity to find and reuse similar queries,
# reducing duplicate API calls and costs by 40-60%.

# Enable semantic caching
SEMANTIC_CACHE_ENABLED=true

# ----------------------------------------------------------------------------
# Embedding Provider Configuration
# ----------------------------------------------------------------------------
# Choose how to generate embeddings for semantic similarity:
# - "local": sentence-transformers (no API key, runs locally)
# - "openai": OpenAI embeddings API (requires API key)
# - "openai-compatible": Ollama, LM Studio, or custom endpoints

# Embedding provider
EMBEDDING_PROVIDER=local

# Embedding model
# For local: sentence-transformers model name (default: all-MiniLM-L6-v2)
# For openai: text-embedding-3-small, text-embedding-3-large, etc.
# For openai-compatible: model name from your endpoint
EMBEDDING_MODEL=all-MiniLM-L6-v2

# API key (only for openai provider)
# EMBEDDING_API_KEY=sk-...

# Base URL (only for openai-compatible provider)
# Examples:
#   - Ollama: http://localhost:11434/v1
#   - LM Studio: http://localhost:1234/v1
# EMBEDDING_BASE_URL=http://localhost:11434/v1

# ----------------------------------------------------------------------------
# Similarity Search Configuration
# ----------------------------------------------------------------------------
# Minimum similarity score for cache hits (0.5-1.0, higher = more strict)
SIMILARITY_THRESHOLD=0.80

# Maximum results to return from similarity search
MAX_SEARCH_RESULTS=10

# ----------------------------------------------------------------------------
# ChromaDB Configuration
# ----------------------------------------------------------------------------
# ChromaDB collection name
CHROMA_COLLECTION_NAME=seraph_semantic_cache

# ChromaDB persistence directory
CHROMA_PERSIST_DIR=./data/chromadb

# Maximum cache entries before cleanup
MAX_CACHE_ENTRIES=10000

# ----------------------------------------------------------------------------
# Performance Settings
# ----------------------------------------------------------------------------
# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=32

# Cache embeddings in memory for faster lookups
CACHE_EMBEDDINGS=true

# ----------------------------------------------------------------------------
# Usage Examples
# ----------------------------------------------------------------------------
# Local embeddings (default, no setup needed):
#   EMBEDDING_PROVIDER=local
#   EMBEDDING_MODEL=all-MiniLM-L6-v2
#
# OpenAI embeddings (high quality, requires API key):
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-small
#   EMBEDDING_API_KEY=sk-...
#
# Ollama embeddings (local, no API key):
#   EMBEDDING_PROVIDER=openai-compatible
#   EMBEDDING_MODEL=nomic-embed-text
#   EMBEDDING_BASE_URL=http://localhost:11434/v1
#
# LM Studio embeddings (local, no API key):
#   EMBEDDING_PROVIDER=openai-compatible
#   EMBEDDING_MODEL=your-model-name
#   EMBEDDING_BASE_URL=http://localhost:1234/v1
